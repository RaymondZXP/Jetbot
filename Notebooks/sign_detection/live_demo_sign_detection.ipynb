{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Detection \n",
    "\n",
    "Great job on completing the first task on collision avoidance! In the previous task, we have used the concept of image classification, collected our own dataset to train a classification model to enable the jetbot to autonomously avoid the obstacles in its path. In this notebook we'll use the same concept to let jetbot follow the signs it sees.\n",
    "\n",
    "This time, we will provide you with a pre-trained model ``best_model_sign_detection`` that can classify the images inpput into five classes: ``free``,``blocked``,``stop``,``left``, and ``right``.Your task is to programme the behaviour of the jetbot based on the classification results. \n",
    "\n",
    "\n",
    "\n",
    "## Station Objective\n",
    "\n",
    "Programme the behaviour of the jetbot based on the image classification result to achieve:\n",
    "    1. Go straight when the classification result is ``free``\n",
    "    2. Slow down when the classification result is ``blocked``\n",
    "    3. Stop when the classification result is ``stop``\n",
    "    4. Turn left/right based on the classification result\n",
    "\n",
    "\n",
    "### Loading the tourch libriries\n",
    "\n",
    "\n",
    "Execute the code below to initialize the PyTorch model.  This should look very familiar from the training notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.alexnet(pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we need to load the trained weights from the ``best_model_sign_detection.pth`` file that we provided and transfer the model weights to GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classifier[6] = torch.nn.Linear(model.classifier[6].in_features, 5)\n",
    "model.load_state_dict(torch.load('best_model_sign_detection.pth'))\n",
    "\n",
    "device = torch.device('cuda')\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the preprocessing function\n",
    "\n",
    "We have now loaded our model, but there's a slight issue.  The format that we trained our model doesnt *exactly* match the format of the camera.  To do that, \n",
    "we need to do some *preprocessing*.  This involves the following steps\n",
    "\n",
    "1. Convert from BGR to RGB\n",
    "2. Convert from HWC layout to CHW layout\n",
    "3. Normalize using same parameters as we did during training (our camera provides values in [0, 255] range and training loaded images in [0, 1] range so we need to scale by 255.0\n",
    "4. Transfer the data from CPU memory to GPU memory\n",
    "5. Add a batch dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's start and display our camera.  You should be pretty familiar with this by now.  We'll also create a slider that will display the\n",
    "probability of each classification result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7d2eadc84f742338aa24f68e396ba29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Image(value=b'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00\\xff\\xdb\\x00Câ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import traitlets\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "from jetbot import Camera, bgr8_to_jpeg\n",
    "\n",
    "camera = Camera.instance(width=224, height=224)\n",
    "image = widgets.Image(format='jpeg', width=224, height=224)\n",
    "\n",
    "blocked_slider = widgets.FloatSlider(description='blocked', min=0.0, max=1.0, orientation='vertical')\n",
    "free_slider = widgets.FloatSlider(description='free', min=0.0, max=1.0, orientation='vertical')\n",
    "stop_slider = widgets.FloatSlider(description='stop', min=0.0, max=1.0, orientation='vertical')\n",
    "left_slider = widgets.FloatSlider(description='left', min=0.0, max=1.0, orientation='vertical')\n",
    "right_slider = widgets.FloatSlider(description='right', min=0.0, max=1.0, orientation='vertical')\n",
    "\n",
    "\n",
    "camera_link = traitlets.dlink((camera, 'value'), (image, 'value'), transform=bgr8_to_jpeg)\n",
    "\n",
    "display(widgets.HBox([image, blocked_slider,free_slider, stop_slider, left_slider,right_slider]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also create our robot instance which we'll need to drive the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programme JetBot Behaviour\n",
    "\n",
    "Now, similar to the function we have used in collision avoidance, just change a few lines of code to enable the jetbot to follow the signs!\n",
    "\n",
    "Try to understand the similar function in the previous notebook, and implement your own logic here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def update(change):\n",
    "    global blocked_slider, robot\n",
    "    x = change['new'] \n",
    "    x = preprocess(x)\n",
    "    y = model(x)\n",
    "    \n",
    "    # we apply the `softmax` function to normalize the output vector so it sums to 1 (which makes it a probability distribution)\n",
    "    y = F.softmax(y, dim=1)\n",
    "\n",
    "    \n",
    "    \n",
    "#################################### Start of user modification ###########################################\n",
    "## insert your code here    \n",
    "########## the standard answer is here\n",
    "    prob_blocked = float(y.flatten()[0])\n",
    "    prob_free = float(y.flatten()[1])\n",
    "    prob_stop = float(y.flatten()[2])\n",
    "    prob_right = float(y.flatten()[3])\n",
    "    prob_left = float(y.flatten()[4])\n",
    "\n",
    "    \n",
    "    blocked_slider.value = prob_blocked\n",
    "    free_slider.value = prob_free\n",
    "    left_slider.value = prob_left\n",
    "    stop_slider.value =prob_stop\n",
    "    right_slider.value =prob_right\n",
    "\n",
    "    \n",
    "     \n",
    "    max_prob =max(prob_blocked,prob_free,prob_left,prob_stop,prob_right)\n",
    "\n",
    "    \n",
    "    if max_prob == prob_blocked:\n",
    "        robot.forward(0.08)\n",
    "    elif max_prob == prob_free:\n",
    "        robot.forward(0.15)\n",
    "    elif max_prob == prob_left:\n",
    "        robot.left(0.15)\n",
    "    elif max_prob == prob_right:\n",
    "        robot.right(0.15)\n",
    "    else:\n",
    "        robot.forward(0)\n",
    "########### end of standard answer\n",
    "############################ End of User modification ########################################\n",
    "    \n",
    "    time.sleep(0.001)\n",
    "        \n",
    "update({'new': camera.value})  # we call the function once to intialize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool! We've created our neural network execution function, but now we need to attach it to the camera for processing. \n",
    "\n",
    "We accomplish that with the ``observe`` function.\n",
    "\n",
    "> WARNING: This code will move the robot!! Please make sure your robot has clearance.  The collision avoidance should work, but the neural\n",
    "> network is only as good as the data it's trained on!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.observe(update, names='value')  # this attaches the 'update' function to the 'value' traitlet of our camera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! If your robot is plugged in it should now be generating new commands with each new camera frame.  Perhaps start by placing your robot on the ground and seeing what it does when it reaches an obstacle.\n",
    "\n",
    "If you want to stop this behavior, you can unattach this callback by executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robot.stop()\n",
    "camera.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
