{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Following Tutorial\n",
    "\n",
    "<font color='red'>\n",
    "Note for next batch of interns: As this tutorial is prepared for typically new users, only small part of the code should be allowed for the users to be edited because of the time restriction of the event. You can handover a psuedocode/ basic idea for object following and let the users edit it within a time frame. I have indicated the part which you can consider giving it to the users for playing with.</font> \n",
    "<a href=\"https://youtu.be/72aoHh-E9EE\">Sample Video Link</a>\n",
    "\n",
    "\n",
    "This tutorial involves a sequence of steps for object following using Jetbot! A pre-trained neural network\n",
    "that was trained on the [COCO dataset](http://cocodataset.org) to detect 90 different common objects is used for this tutorial.  These include\n",
    "\n",
    "* Person (index 0)\n",
    "* Cup (index 47)\n",
    "\n",
    "and many others (you can check [this file](https://github.com/tensorflow/models/blob/master/research/object_detection/data/mscoco_complete_label_map.pbtxt) for a full list of class indices).  Common objects which work very well with the Jetbot are person(index 0), cup(index 47) and bottle(index 44). The model is sourced from the [TensorFlow object detection API](https://github.com/tensorflow/models/tree/master/research/object_detection),\n",
    "which provides utilities for training object detectors for custom tasks also!  Once the model is trained, we optimize it using NVIDIA TensorRT on the Jetson Nano.\n",
    "\n",
    "This makes the network very fast, capable of real-time execution on Jetson Nano!  We won't run through all of the training and optimization steps in this notebook though.\n",
    "\n",
    "Anyways, let's get started.  First, we'll want to import the ``ObjectDetector`` class which takes our pre-trained SSD engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from jetbot import ObjectDetector\n",
    "\n",
    "## this will take a while to load\n",
    "model = ObjectDetector('ssd_mobilenet_v2_coco.engine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, the ``ObjectDetector`` class uses the TensorRT Python API to execute the engine that we provide.  It also takes care of preprocessing the input to the neural network, as\n",
    "well as parsing the detected objects.  Right now it will only work for engines created using the ``jetbot.ssd_tensorrt`` package. That package has the utilities for converting\n",
    "the model from the TensorFlow object detection API to an optimized TensorRT engine.\n",
    "\n",
    "Next, let's initialize our camera and widgets.  Our detector takes 300x300 pixel input, so we'll set this when creating the camera.\n",
    "\n",
    "> Internally, the Camera class uses GStreamer to take advantage of Jetson Nano's Image Signal Processor (ISP).  This is super fast and offloads\n",
    "> a lot of the resizing computation from the CPU. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Camera\n",
    "from IPython.display import display\n",
    "import ipywidgets.widgets as widgets\n",
    "\n",
    "camera = Camera.instance(width=300, height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Control robot to follow central object\n",
    "\n",
    "Now we want our robot to follow an object of the specified class.  To do this we'll do the following\n",
    "\n",
    "1.  Detect objects matching the specified class\n",
    "2.  Select object closest to center of camera's field of vision, this is the 'target' object\n",
    "3.  Steer robot towards target object, otherwise wander\n",
    "4.  If we're blocked by an obstacle, turn left\n",
    "\n",
    "We'll also create some widgets that we'll use to control the target object label, the robot speed, and\n",
    "a \"turn gain\", that will control how fast the robot turns based off the distance between the target object\n",
    "and the center of the robot's field of view. \n",
    "\n",
    "\n",
    "First, let's load our collision detection model.  The pre-trained model is stored in this directory as a convenience, but if you followed\n",
    "the collision avoidance example you may want to use that model if it's better tuned for your robot's environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "collision_model = torchvision.models.alexnet(pretrained=False)\n",
    "\n",
    "\n",
    "#################### loading of collision avoidance model ##########################################\n",
    "### The following two lines will load the collision avoidance model, but this will consume additional CPU and we do not encourage to have it. \n",
    "### Uncomment the next two lines to try with the collision avoidance models implemented as well\n",
    "\n",
    "# collision_model.classifier[6] = torch.nn.Linear(collision_model.classifier[6].in_features, 2)\n",
    "# collision_model.load_state_dict(torch.load('../collision_avoidance/best_model_alex.pth'))\n",
    "\n",
    "#################### End of the loading the collision avoidance model ##############################\n",
    "\n",
    "device = torch.device('cuda')\n",
    "collision_model = collision_model.to(device)\n",
    "\n",
    "mean = 255.0 * np.array([0.485, 0.456, 0.406])\n",
    "stdev = 255.0 * np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "normalize = torchvision.transforms.Normalize(mean, stdev)\n",
    "\n",
    "def preprocess(camera_value):\n",
    "    global device, normalize\n",
    "    x = camera_value\n",
    "    x = cv2.resize(x, (224, 224))\n",
    "    x = cv2.cvtColor(x, cv2.COLOR_BGR2RGB)\n",
    "    x = x.transpose((2, 0, 1))\n",
    "    x = torch.from_numpy(x).float()\n",
    "    x = normalize(x)\n",
    "    x = x.to(device)\n",
    "    x = x[None, ...]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now let's initialize our robot so we can control the motors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jetbot import Robot\n",
    "\n",
    "robot = Robot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's display all the control widgets and connect the network execution function to the camera updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a92f21f494647b8b196182ad79f765b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Image(value=b'', format='jpeg', height='300', width='300'), FloatSlider(value=0.â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from jetbot import bgr8_to_jpeg\n",
    "\n",
    "blocked_widget = widgets.FloatSlider(min=0.0, max=1.0, value=0.0, description='blocked')\n",
    "image_widget = widgets.Image(format='jpeg', width=300, height=300)\n",
    "label_widget = widgets.IntText(value=47, description='tracked label')\n",
    "speed_widget = widgets.FloatSlider(value=0.2, min=0.0, max=1.0, description='speed')\n",
    "turn_gain_widget = widgets.FloatSlider(value=0.25, min=0.0, max=2.0, description='turn gain')\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([image_widget, blocked_widget]),\n",
    "    label_widget,\n",
    "    speed_widget,\n",
    "    turn_gain_widget\n",
    "]))\n",
    "\n",
    "width = int(image_widget.width)\n",
    "height = int(image_widget.height)\n",
    "\n",
    "def detection_center(detection):\n",
    "    \"\"\"Computes the center x, y coordinates of the object\"\"\"\n",
    "    bbox = detection['bbox']\n",
    "    center_x = (bbox[0] + bbox[2]) / 2.0 - 0.5\n",
    "    center_y = (bbox[1] + bbox[3]) / 2.0 - 0.5\n",
    "    return (center_x, center_y)\n",
    "    \n",
    "def norm(vec):\n",
    "    \"\"\"Computes the length of the 2D vector\"\"\"\n",
    "    return np.sqrt(vec[0]**2 + vec[1]**2)\n",
    "\n",
    "def closest_detection(detections):\n",
    "    \"\"\"Finds the detection closest to the image center\"\"\"\n",
    "    closest_detection = None\n",
    "    for det in detections:\n",
    "        center = detection_center(det)\n",
    "        if closest_detection is None:\n",
    "            closest_detection = det\n",
    "        elif norm(detection_center(det)) < norm(detection_center(closest_detection)):\n",
    "            closest_detection = det\n",
    "    return closest_detection\n",
    "        \n",
    "def execute(change):\n",
    "    image = change['new']\n",
    "    \n",
    "    # execute collision model to determine if blocked\n",
    "    collision_output = collision_model(preprocess(image)).detach().cpu()\n",
    "    prob_blocked = float(F.softmax(collision_output.flatten(), dim=0)[0])\n",
    "    blocked_widget.value = prob_blocked\n",
    "    \n",
    "    ##### Collision avoidance didn't work very well and it is not our main objective of this tutorial, thus I have commented it. ###############\n",
    "    # Consider using the model in the object avoidance example, if necessary \n",
    "    # # turn left if blocked\n",
    "    #     if prob_blocked > 0.95:\n",
    "    #     robot.left(0.1)\n",
    "    #     image_widget.value = bgr8_to_jpeg(image)\n",
    "    #     return\n",
    "    ######################################################################################################################################\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    # compute all detected objects\n",
    "    detections = model(image)\n",
    "    \n",
    "    # draw all detections on image\n",
    "    for det in detections[0]:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (255, 0, 0), 2)\n",
    "    \n",
    "    # select detections that match selected class label\n",
    "    matching_detections = [d for d in detections[0] if d['label'] == int(label_widget.value)]\n",
    "    \n",
    "    # get detection closest to center of field of view and draw it\n",
    "    det = closest_detection(matching_detections)\n",
    "    if det is not None:\n",
    "        bbox = det['bbox']\n",
    "        cv2.rectangle(image, (int(width * bbox[0]), int(height * bbox[1])), (int(width * bbox[2]), int(height * bbox[3])), (0, 255, 0), 5)\n",
    "    \n",
    "    ##########################################start of user modification ##################################################\n",
    "    #### User can work with the following logic and interpret in other ways to perform object following function(Preferable if a psuedocode is given)\n",
    "    #Do not move until specific object is detected\n",
    "    if det is None:\n",
    "        robot.set_motors(0,0)\n",
    "        \n",
    "    #Otherwise steer towards target\n",
    "    else:\n",
    "        # move robot forward and steer proportional target's x-distance & y- distance from the center\n",
    "        center = detection_center(det)\n",
    "        if abs(center[1]) < 0.1:\n",
    "            speed = 0\n",
    "        else:\n",
    "            speed = abs(speed_widget.value * 3 * center[1])\n",
    "            robot.set_motors(\n",
    "            float(speed + turn_gain_widget.value * center[0]),\n",
    "            float(speed - turn_gain_widget.value * center[0])\n",
    "            )\n",
    "    ############################################# end of user modification ########################################\n",
    "    \n",
    "    # update image widget\n",
    "    image_widget.value = bgr8_to_jpeg(image)\n",
    "    \n",
    "execute({'new': camera.value})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the block below to connect the execute function to each camera frame update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.unobserve_all()\n",
    "camera.observe(execute, names='value')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!  If the robot is not blocked you should see boxes drawn around the detected objects in blue.  The target object (which the robot follows) will be displayed in green.\n",
    "\n",
    "The robot should steer towards the target when it is detected.  If it is blocked by an object it will simply turn left.\n",
    "\n",
    "You can call the code block below to manually disconnect the processing from the camera and stop the robot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## If you are starting a new kernel that needs to use the camera, it is important to run these lines\n",
    "import time\n",
    "camera.unobserve_all()\n",
    "time.sleep(1.0)\n",
    "robot.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following step is added to terminate the camera before shut down. Useful step if the camera often hangs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
